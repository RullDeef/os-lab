# Прерывания

Рассматриваем механизмы обслуживания прерываний в системе.

Простейшую схему получения прерываний процессором мы рассматривали. В совр системах она не работает. В совр системах информирование процессора выполняется с помощью MSI (message signal interrupt). Это серьезная аппаратная поддержка составляющей системы.

В системе аппаратные прерывания делятся на быстрые и медленные. Это классическое деление с точки зрения их обслуживания системой. Но в современных системах существует 1 быстрое аппаратное прерывание - от системного таймера. Обработчки быстрых прерываний выполняются от начала до конца. Медленные - это все остальные прерывания (от внешних устройств). Обработчики прерываний от внешних устройств реализуются в виде 2 частей (исторически - `top half` & `bottom half`). Верхняя половина выполняется на высочайшем уровне приоритета. На процессоре запрещены все прерывания в момент выполнения, а для других процессоров запрещены все прерывания по этой линии IRQ.

Обработчик аппаратного прерывания выполняет только самый необходимый обьем действий - получение данных с соответствующего порта, к которому подключено внешнее устройство и сохранение этих данных в буфер ядра. Кроме этого он должен инициализировать работу отложенного действия - второй половины. И эта половина может быть реализована в виде `softirq`, `tasklet`а или очереди работ.

Каждый из перечисленных типов обладает особенностями, которые необходимо учитывать при выборе реализаци механизма отложенных действий. (рассмотрели `softirq`). `tasklet`ы - вид `softirq`.

Поскольку именно сетевая подсистема способна воспр большое колво пакетов, их может быть очень большое колво.

Демон `ksoftirqd` это поток ядра каждого процессора (per CPU). задача которого входит планирование и запуск на выполнение `softirq`.

Когда машина нагружена гибкими прерываниями которые обслуживаются при завершении аппаратного прерывания. При этом таких аапаратных прерываний может возникать очень много, каждый инициирует свой обработчик, в результате - очередь из softirq. При этом например по размеру очереди или по времени работы ksoftirqd можно определить степень давления на систему этих обращений подсетевого устройства. По колву времени можно определить величину нагрузки на систему от сетевой подсистемы.

Очевидно, что для того, чтобы управлять выполнением отложенных действий как раз предназначены такие демоны.

---

## taskletы

является частным случаем реализации softirq. Но в отличае от softirq, которые пишутся таким обазом чтобы одновременно в системе могло выполнятся некот колво одних и тех же softirq, то на такслеты наклад ограничения: обработчик можетвыполняться только на одном процессоре (один не может выполняться параллельно, разные - могут). В системах в которых требуется быстрая реакция - предпочтениеотдается softirq. Но реализация taskletа намного проще.

Указывается, что tasklet является хорошим компромиссом между производительностью системы и простотой использования.

В отличае от softirq которые регистр при компиляции системы и их колво опр в системе, tasklet может быть зареган как статически так и динамически.

tasklet описывается структурой:

```C
struct tasklet_struct {
    struct tasklet_struct *next;
    unsigned long state; // опрделяет состояние
    atomic_t count;
    bool use_callback;
    union {
        void (*func)(unsigned long data);
        void (*callback)(struct tasklet_struct *t);
    };
    unsigned long data;
};
```

```С
enum {
    TASKLET_STATE_SCHED,
    TASKLET_STATE_RUN
};
```

Tasklet может быть обьявлен статически и динамически

Статически с помощью макроса:

```C
#include <linux/interrupts.h>

DECLARE_TASKLET(name, func, data);
DECLARE_TASKLET_DISABLED(name, func, data);

---

#define DECLARE_TASKLET(name, __callback) \
struct tasklet_struct name = { \
    .count = ATOMIC_INIT(0), \
    .callback = __callback, \
    .use_callback = true }
```

Динамически с помощью функции `tasklet_init`:

```C
extern void tasklet_init(struct tasklet_struct *t, void (*func)(unsigned long), unsigned long data);
```

---

## Планирование tasklet'ов

Обработчи прерывания должен запланировать отложенное действие. Для этого система предоставляет функции:

```C
extern void __tasklet_schedule(struct tasklet_struct *t);
static inline void tasklet_shedule(struct tasklet_struct *t)
{
    if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))
        __tasklet_schedule(t);
}

extern void __tasklet_hi_schedule(struct tasklet_struct *t);
static inline void tasklet_hi_shedule(struct tasklet_struct *t)
{
    if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))
        __tasklet_hi_schedule(t);
}
```

* `tasklet_disable_nosync(...)`
* `tasklet_disable(...)`
* `tasklet_enable(...)`

```C
extern void tasklet_kill(struct tasklet_struct *t);
extern void tasklet_kill_immediate(struct tasklet_Struct *t, unsigned int cpu);
```

---

## tasklet handler

```C
void tasklet_handler(unsigned long data);
```

taskletы не могут блокироваться. Нельзя использовать блокировки (например семафоры). Если в нем исп общие данные, то необходимо использовать spin-lockи.

Свойства tasklet'ов:

1. Если вызывается функция `tasklet_sheduler` то tasklet гарантировано будет выполняться на каком-то процессоре хотя бы один раз после этого.

2. Если тасклет уже запланирован, но его выполнение еще не началось, то он будет один раз выполнен обязательно.

3. Если тасклет уже выполняется на другом процессоре или планирование тасклета вызвано из самого тасклета, то его перепланирование будет отложено.

4. Если один и тот же тасклет выполняется несколько раз, то для монопольного доступа к разделяемым данным необходимо использовать спинлоки.

В системе имеется определенная на тасклетах функция блокировки

```C
static inline int tasklet_trylock(struct tasklet_struct *t)
{
    return !test_and_set_bit(TASKLET_STATE_RUN, &t->state);
}
```

---

## Очереди работ

В отличие от тасклетов могут блокироваться. Главное отличие - очереди работ реализованы совершенно по-другому.

Наибольшие изменения произошли с параллельным выполнением очередей работ (SMP).

`!важно!` *Несколько концепций очередей работ представляют собой связанные с работой структурой данных, с которой легко перепутать*

Что имеется в виду?

1. `work` - работа
2. `workqueue` - очередь работ. При этом в одну очередь работ может быть поставлено много работ. Т.е. работа связывается с конкретной очередью работ.
3. В системе определены так называемые `worker`ы - это поток ядра `work_thread`.
4. `worker_pool`. В системе имеется пул рабочих.
5. `pwq` (`pool_workqueue`). Так называемый посредник. Ответственный за установку отношений между `workqueue` и `worker_pool`.

`CMWQ` - concurrent managed work queue

Для очередей работ очень важно четко представлять, какой материал изучать. Они очень сильно были переписаны.

Существенное отличие между тасклетами и очередями работ:

1. Тасклеты вып в контексте прерывания. В результате чего код тасклета должен быть неделимим. В отличае от тасклетов, очереди работ выполняются в контексте спец потоков ядра. И как результат имеют большую свободу действий. И в частности очереди работ могут блокироваться (засыпать).
2. Тасклеты всегда выполняются на процессоре, на котором выполнялось прерывание, запланировавшее данный тасклет. Очереди работ по умолчанию также выполняются на том же процессоре, но могут выполняться и на других. Код ядра требует, чтобы выполнения функций очереди работ откладывались на определенный интервал времени.

Ключевым отличием является то, что тасклеты выполняются за которкий период времени, а очереди работ имеют значительно большие задержки и необязательно должны быть неделимыми.

Структуры:

```C
struct workqueue_struct {
    struct list_head pwqs;
    struct list_head list; // список всех очередей работ
    struct mutex mutex;
    ...
    char name[WQ_NAME_LEN];
    struct rcu_head rcu; // на свой ЦПУ список работ
};

struct work_struct {
    atomic_long_t data;
    struct list_head entry;
    work_func_t func;
    #ifdef CONFIG_LOCKDEP
        struct lockdep_map lockdep_map;
    #endif
}
```

также как для тасклетов, работу можно поместить в очередь работ как статически так и динамически. Для этого предлагаются соотв средства:

```C
DECLARE_WORK(name, void (*func)(void*));
// name - имя структуры work_struct
```

Или динамически. И тогда в системе имеется 2 макроса:

```C
#ifdef CONFIG_LOCKDEP
#define __INIT_WORK(__work, __func, __onstack)
    do {
        static struct lock_class_key __key;
        __init_work((__work), __onstack);
        (__work)->data = (atomic_long_t) WORK_DATA_INIT();
        INIT_LIST_HEAD(&(__work)->entry);
        __work->func = __func;
    } while (0);
#endif
#define INIT_WORK(__work, __func)
    __INIT_WORK(__work, __func, 0)
```

```C
typedef void (*work_func_t)(struct work_struct *work);
```

```C
PREPARE_WORK()
```

если имеется хоть какая-то возможность, что структура описыв очередь работ уже инициализирована (представлена в системе), лучше вместо `init_work` использовать `prepare_work`.

Для создания очереди работ использовалась функция `create_workqueue` (2.6.36). В более современных версиях ядра:

```C
int alloc_workqueue(char *name, unsigned int flags, int max_active);
```

* name - имя очереди. В отличие от старых реализаций, в системе не будет создано потоков с таким именем.
* flags - определяет, как очередь работ будет выполняться
* max_active - ограничивает число задач, которые одновременно могут стоять в очереди к процессору.

Определены следующие флаги:

```C
enum {
    WQ_UNBOUND = 1 << 1,
    WQ_FREEZEABLE = 1 << 2,
    // продолжение следует
};
```
